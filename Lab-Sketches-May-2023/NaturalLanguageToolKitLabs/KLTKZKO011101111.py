""" < - introduction to main module click to get a an overview
ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ†

                â–ˆâ–„â–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–‘â–‘â€ƒ â€ƒâ–ˆâ–‘â–‘â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€â€ƒ â€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–‘â€ƒ â€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ€ƒâ–€â–ˆâ–€
                â–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–„â€ƒ â€ƒâ–ˆâ–„â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒ â€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–„â–„â€ƒ â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–‘â–ˆâ–‘

                                                tÌ´Ì½Í€Ì’Í“ÍrÌ´ÍŠÍ„Í Ì¡Í–aÌ´ÍÍÍ”Í•Ì»nÌ¸Ì¿Ì”ÍÍ“Í•ÌºsÌ´Í„Ì¾Í„Í‡ÌŸÍŽlÌ´ÍÍÍ„ÍÌ¢Ì¼aÌ¸Í›ÍŠÍÍ™Ì Í“tÌ¸ÌšÍ›Í˜Í™Í”Í•eÌ¸Í†Í›Ì”ÌºÌ¼ iÌ¸ÍƒÍÌ”ÌªÌªtÌ´ÍŠÌšÍŒÍ‡ÌŸÌž fÌ¸ÍÌ•Ì”Ì«Ì™Ì˜oÌ´Í˜Í›Ì«Í•Ì¦rÌ´Í›Í‹Ì¢Í™ mÌ¸Í„Ì’Ì’Ì»ÍœeÌ¸Í›Í„ÍƒÌªÍ–Ì¼

                                                â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                                        â£¤â£¶â£¶â£¶â£¶â£¶â£¦â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢°â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â €â£ â¢¿â£¿â£¿â¡¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£€â €â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â£°â£¿â£¿â£¿â£¿â¡‡â£¿â£·â£¿â£¿â£¿â£¿â£¿â£¿â£¯â¡„â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â¡°â£¿â£¿â£¿â£‡â£¿â£€â ¸â¡Ÿâ¢¹â£¿â£¿â£¿â£¿â£¿â£¿â£·â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â¢€â¢¡â£¿â£¿â£¿â¡‡â â ‹â €â €â €â¢¿â¢¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â ¸â¢¸â ¸â£¿â£¿â£‡â €â €â €â €â €â €â Šâ£½â£¿â£¿â£¿â â£·â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¿â£¿â£¿â£·â£„â €â €â €â¢ â£´â£¿â£¿â£¿â ‹â£ â¡â¡„â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â â ¾â£¿â£Ÿâ¡»â ‰â €â €â €â ˆâ¢¿â ‹â£¿â¡¿â šâ ‹â â¡â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â¢€â£´â£¶â£¾â£¿â£¿â¡„â €â£³â¡¶â¡¦â¡€â£¿â£¿â£·â£¶â£¤â¡¾â â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¿â¡†â €â¡‡â¡¿â ‰â£ºâ£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¯â ½â¢²â ‡â £â â šâ¢»â£¿â£¿â£¿â €â €â €â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â£¿â£¿â£¿â£¿â£¿â¡â£¾â¡â£·â €â €â£¼â£·â¡§â£¿â£¿â£¦â£„â¡€â €â €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â €â£»â£¿â£¿â£¿â£¿â£¿â£®â ³â£¿â£‡â¢ˆâ£¿â Ÿâ£¬â£¿â£¿â£¿â£¿â£¿â¡¦â¢„â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â¢€â¢„â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£œâ¢¿â£¼â¢â£¾â£¿â£¿â£¿â¢»â£¿â£â£¿â£¦â¡‘â¢„â €â €â €â €â €
                                    â €â €â €â €â €â €â €â£ â£¶â£·â£¿â£¿â ƒâ ˜â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡·â£¥â£¿â£¿â£¿â£¿â£¿â €â ¹â£¿â£¿â£¿â£·â¡€â €â €â €â €â €
                                    â €â €â €â €â£‡â£¤â£¾â£¿â£¿â¡¿â »â¡â €â €â ¸â£¿â£¿â£¿â£¿â£¿â£¿â£®â£¾â£¿â£¿â£¿â£¿â¡‡â €â €â ™â£¿â£¿â¡¿â¡‡â €â €â €â €â €
                                    â €â €â¢€â¡´â£«â£¿â£¿â£¿â ‹â €â €â¡‡â €â €â¢°â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡‡â €â €â €â¢˜â£¿â£¿â£Ÿâ¢¦â¡¸â €â €â €
                                    â €â¡°â ‹â£´â£¿â£Ÿâ£¿â ƒâ €â €â €â ˆâ €â €â£¸â£¿â£¿â£¿â£¿â£¿â£¿â£‡â£½â£¿â£¿â£¿â£¿â£‡â €â €â €â â ¸â£¿â¢»â£¦â ‰â¢†â €â €
                                    â¢ â ‡â¡”â£¿â â â ™â †â €â €â €â €â¢€â£œâ£›â¡»â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡¿â¡€â €â €â €â €â¡‡â¡‡â ¹â£·â¡ˆâ¡„â €
                                    â €â¡¸â£´â¡â €â €â €â €â €â €â €â¢€â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£»â£¿â£¿â£¿â£¿â£¿â£¿â¡„â €â €â €â¡‡â¡‡â €â¢»â¡¿â¡‡â €
                                    â €â£¿â£¿â£†â €â €â €â €â €â €â¢€â£¼â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡€â €â£°â ¿â ¤â ’â¡›â¢¹â£¿â „
                                    â €â£¿â£·â¡†â â €â €â €â €â¢ â£¿â£¿â Ÿâ »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡Ÿâ »â¢·â¡€â €â €â €â €â €â£¸â£¿â €
                                    â €â ˆâ ¿â¢¿â£„â €â €â €â¢žâ Œâ¡¹â â €â €â¢»â¡‡â ¹â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â â¢³â €â €â â €â €â €â €â¢ â£¿â¡â €
                                    â €â €â €â ˆâ ‚â €â €â €â ˆâ£¿â â €â €â €â¡‡â â €â ˜â¢¿â£¿â£¿â ¿â Ÿâ ‹â ›â ›â ›â €â¢¸â €â €â¡€â ‚â €â €â â ›â ‰â €â €
                                    â €â €â €â €â €â €â €â €â €â â •â£ â¡„â£°â¡‡â €â €â €â¢¸â£§â €â €â €â €â €â €â €â¢€â£¸â  â¡ªâ Šâ €â €â €â €â €â €â €â €
                                    â €â €â €â €â €â €â €â €â €â €â €â ˆâ¢«â£½â¡‹â ­â ¶â ®â¢½â£¿â£†â €â €â €â €â¢ â£¿â£“â£½â â €â €â €â €â €â €â €â €â €â €
                                    
                                        devoted to the code and excel into the software

NLTK is a library that allows you to compile natrual language within python modules to translate , tokenize, parse, stem , tag the data
you will be translating into machine code!

ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ† ðŸ…¶ðŸ…´ðŸ…½ðŸ…´ðŸ†‚ðŸ…¸ðŸ†‚ðŸ…¶ðŸ…¸ðŸ†
"""

# import NLTK
import nltk
from typing import Type

# TODO when running NLTK for the first time you must run the following code to install the dependant packages!
#nltk.download()

# important methods and shit
def openfile() -> Type[None]:
    
    # docstring
    ''' openfile() method
    open a designated file provided by user or the programmer
    '''
    
    # global
    global sentencedata
    
    # file path to text file
    file_path = r'C:\Users\daint\Documents\GitHub\Scrap-Yard\Lab-Sketches-May-2023\NaturalLanguageToolKitLabs\data\data.txt'

    # open file containing data
    with open(file=file_path, mode='r') as f:
        
        # store data within var
        sentencedata = f.read()

# TODO create a method that can handle all ways to tokenize input
def more_efficient(token_type: str) -> Type[None]:
    
    # docstring
    ''' # more_efficient(token_type: str) method
    handles every type of algorithm of tokenizers in one cohesive function that takes one parameter 'token_type' which is of
    the data type of a str.
    '''
    pass

# tokenizes words
def otaku_ahrii_word_tokenizer(user_data: str = None) -> Type[str]:
    
    """ < - otaku_ahrii_word_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
            â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–ˆâ–‘â–ˆâ–‘â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„    â–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ â–„â–€â€ƒâ–€â–„
            â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ    â–€â–„â–€â–„â–€â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–„â–€   â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„ â–€â–„â€ƒâ–„â–€

                                        â£¿â£¿â£¿â£¿â£¿â£¿â Ÿâ ‹â â£€â£¤â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ¢¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â ‹â â €â €â ºâ ¿â¢¿â£¿â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˜â »â£¿
                                        â£¿â£¿â¡Ÿâ â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£¤â£¤â£¤â£¤â €â €â €â €â €â£¤â£¦â£„â €â €
                                        â£¿â¡Ÿâ €â €â €â €â €â €â €â €â €â €â¢€â£¤â£¶â£¿â â£¿â£¿â£¿â£¿â£¿â£â €â €â €â ›â ™â ›â ‹â €â €
                                        â¡¿â €â €â €â €â €â €â €â €â¡€â €â£°â£¿â£¿â£¿â£¿â¡„â ˜â£¿â£¿â£¿â£¿â£·â „â €â €â €â €â €â €â €â €
                                        â¡‡â €â €â €â €â €â €â €â ¸â ‡â£¼â£¿â£¿â£¿â£¿â£¿â£·â£„â ˜â¢¿â£¿â£¿â£¿â£…â €â €â €â €â €â €â €â €
                                        â â €â €â €â£´â£¿â €â£â££â£¸â£¿â£¿â£¿â£¿â£¿â Ÿâ ›â ›â €â Œâ »â£¿â£¿â£¿â¡„â €â €â €â €â €â €â €
                                        â €â €â €â£¶â£®â£½â£°â£¿â¡¿â¢¿â£¿â£¿â£¿â£¿â£¿â¡€â¢¿â£¤â „â¢ â£„â¢¹â£¿â£¿â£¿â¡†â €â €â €â €â €â €
        sÍ›ysÍ›á´›â·®eÍ¤mÍ«aÍ£á´›â·®iÍ¥cÍ¨ rÍ¬eÍ¤aÍ£sÍ›oÍ¦niÍ¥ng            â €â €â €â£¿â£¿â£¿â£¿â£¿â¡˜â£¿â£¿â£¿â£¿â£¿â£¿â ¿â£¶â£¶â£¾â£¿â£¿â¡†â¢»â£¿â£¿â ƒâ¢ â –â ›â£›â£·â €            YASSSSSSSSS WORDSSSSSS
            ðŸ‡²â€‹â€‹â€‹â€‹â€‹ðŸ‡¦â€‹â€‹â€‹â€‹â€‹ðŸ‡¨â€‹â€‹â€‹â€‹â€‹ðŸ‡­â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ ðŸ‡¨â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡©â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹                    â£¿â£¿â£¿â£¿â£¿â£¿â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£®â£â¡»â ¿â ¿â¢ƒâ£„â£­â¡Ÿâ¢€â¡Žâ£°â¡¶â£ªâ£¿â €
                                        â €â €â ˜â£¿â£¿â£¿â Ÿâ£›â »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¿â£¿â£¿â¡¿â¢â£¾â£¿â¢¿â£¿â£¿â â €
                                        â €â €â €â£»â£¿â¡Ÿâ ˜â ¿â ¿â Žâ »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£µâ£¿â£¿â §â£·â Ÿâ â €â €
                                        â¡‡â €â €â¢¹â£¿â¡§â €â¡€â €â£€â €â ¹â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‹â¢°â£¿â €â €â €â €
                                        â¡‡â €â €â €â¢»â¢°â£¿â£¶â£¿â¡¿â ¿â¢‚â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â£»â£¿â£¿â£¿â¡â €â €â â €â €â €â €        ð–ˆð–”ð–’ð–•ð–Žð–‘ð–Š, ð–ˆð–”ð–’ð–•ð–Žð–‘ð–Š, 01010101010101
                                        â£·â €â €â €â €â ˆâ ¿â Ÿâ£â£´â£¾â£¿â£¿â ¿â ¿â£›â£‹â£¥â£¶â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â € â£¿â¡€
    
    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    The otaku_ahrii_tokenizer() is a function built to tokenize words into tokens and is a feature within the module that needs to be fed
    a positional argument in order for it to work! This can be easy as scripting a value into the corrosponding function call and send it to the
    parmeter 

    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """
    
    # import nltk
    import nltk
    
    # tokenize this thing
    SEXY_TOKENS = nltk.word_tokenize(sentencedata)

    # display sentence tokens
    print('The data you have provided has been tokenized via using the nltk module, the data senetnces now have their own strings:')
    return print(SEXY_TOKENS, '\n')

# tokenizes sentences
def otaku_ahrii_sentence_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_sentence_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
        â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ€ƒ â–ˆâ–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€ â€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ â–„â–€â€ƒâ–€â–„
        â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ€ƒ â–„â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ–„â–„â€ƒâ–ˆâ–ˆâ–„â–‘  â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„ â–€â–„â€ƒâ–„â–€





                                                    â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡¿â â €â €â €â €â  â ¤â ¶â žâ¢»â£¿â¡¿â£¿â£¿â£¿â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â Ÿâ â €â¢€â£ â£¤â£¤â£´â£¶â£„â €â¢¸â£¿â ‡â »â£¿â£¿â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‹â €â €â °â ›â ›â ›â »â ¿â ¿â£¿â¡‡â ˆâ ‰â €â €â ˆâ »â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢ â£¤â£„â¡€â¢¹â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â¢â¡œâ¢±â €â €â €â €â €â €â €â €â €â €â €â €â €â ¾â¢¿â£¿â »â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â¡¿â¢¸â¡žâ£ â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ›â €â €â ¹â¡
                                        â£‰â ™â »â£¿â£¹â¡‡â¡žâ¢°â¡Ÿâ €â£ â ¤â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¹
                                        â ˆâ¡¹â£¶â¢†â£¿â¢±â¡‡â¢¸â¡·â ‹â£ â£¤â¡ˆâ¢‡â €â €â €â €â €â €â €â¢€â¡¤â „â¢ â¡€â €â €â €â ˆ
                                        â €â£‡â£¿â¢¸â£¿â ¸â£·â €â¢§â£¾â ‹â ˆâ »â£¾â£¦â €â €â €â €â €â£´â ‹â¢€â£¦â €â¢¿â €â €â €â¢€
                                        â¡€â ˆâ£¿â ˜â¢¿â „â ˆâ¢€â ¸â¡â €â €â¢°â¡‡â¡œâ €â €â €â €â €â â €â ˆâ¢¸â ˆâ €â €â €â €â¡¼
                                        â£¿â£·â£¿â €â €â €â €â¡Œâ €â¢§â£€â¡´â ›â¢â ƒâ €â €â €â €â €â €â €â €â €â €â €â €â¢€â£¾â¡‡
                                        â£¿â£¿â£¿â¡‡â €â €â °â¢°â €â €â ™â ƒâ¢€â¡¾â €â €â €â €â €â €â €â €â €â €â €â¢€â£´â£¿â£¿â¡‡
                                        â£¿â£¿â£¿â£·â €â €â €â¡¸â €â €â €â£ â£¿â£¿â£¶â£¤â£¤â£€â¡€â €â €â €â €â¢€â£´â£¿â£¿â£¿â£¿â¡‡
                                        â£¿â£¿â£¿â â €â €â¢€â¡‡â¢€â£ â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¶â¡žâ ‹â¢¸â£¿â£¿â£¿â£¿â¡‡
                                        â£¿â¡¿â ƒâ €â â ¶â£¿â¡¿â¢»â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£žâ¢»â£¿â£¿â£¿â£¿â¡‡

    
    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    The otaku_ahrii_sentence_tokenizer() is a function built to tokenize sentences into tokens

    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """
    # import module
    import nltk
    
    # tokenizing stage
    SEXY_TOKENS = nltk.sent_tokenize(sentencedata)
    
    # display sentence tokens stage (optional)
    print('The data you have provided has been tokenized via using the nltk module, the data senetnces now have their own strings:')
    print('%s sentences found within the text file!' %(len(SEXY_TOKENS)))
    print('tokens found:')
    
    # define func()
    def iter():
        
        # iterate through tokens
        for token in SEXY_TOKENS:
            
            print(SEXY_TOKENS[token]) # display token found 
    
    # call func()
    return iter()

# TODO add different artsyle to it
# tokenizes whitespace
def otaku_ahrii_whitepace_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
        â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ€ƒ  â–ˆâ–‘â–ˆâ–‘â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€   â–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ â–„â–€â€ƒâ–€â–„
        â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ€ƒ  â–€â–„â–€â–„â–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–ˆâ–„â€ƒâ–„â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–„â€ƒâ–ˆâ–ˆâ–„    â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„ â–€â–„â€ƒâ–„â–€

                                        
                                                    â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡¿â â €â €â €â €â  â ¤â ¶â žâ¢»â£¿â¡¿â£¿â£¿â£¿â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â Ÿâ â €â¢€â£ â£¤â£¤â£´â£¶â£„â €â¢¸â£¿â ‡â »â£¿â£¿â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‹â €â €â °â ›â ›â ›â »â ¿â ¿â£¿â¡‡â ˆâ ‰â €â €â ˆâ »â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢ â£¤â£„â¡€â¢¹â£¿
                                        â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â¢â¡œâ¢±â €â €â €â €â €â €â €â €â €â €â €â €â €â ¾â¢¿â£¿â »â£¿â£¿â£¿
                                        â£¿â£¿â£¿â£¿â£¿â¡¿â¢¸â¡žâ£ â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ›â €â €â ¹â¡
                                        â£‰â ™â »â£¿â£¹â¡‡â¡žâ¢°â¡Ÿâ €â£ â ¤â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¹
                                        â ˆâ¡¹â£¶â¢†â£¿â¢±â¡‡â¢¸â¡·â ‹â£ â£¤â¡ˆâ¢‡â €â €â €â €â €â €â €â¢€â¡¤â „â¢ â¡€â €â €â €â ˆ
                                        â €â£‡â£¿â¢¸â£¿â ¸â£·â €â¢§â£¾â ‹â ˆâ »â£¾â£¦â €â €â €â €â €â£´â ‹â¢€â£¦â €â¢¿â €â €â €â¢€
                                        â¡€â ˆâ£¿â ˜â¢¿â „â ˆâ¢€â ¸â¡â €â €â¢°â¡‡â¡œâ €â €â €â €â €â â €â ˆâ¢¸â ˆâ €â €â €â €â¡¼
                                        â£¿â£·â£¿â €â €â €â €â¡Œâ €â¢§â£€â¡´â ›â¢â ƒâ €â €â €â €â €â €â €â €â €â €â €â €â¢€â£¾â¡‡    Catch those pesky words and tokeneize them even when spaces, 
                                        â£¿â£¿â£¿â¡‡â €â €â °â¢°â €â €â ™â ƒâ¢€â¡¾â €â €â €â €â €â €â €â €â €â €â €â¢€â£´â£¿â£¿â¡‡    tabs, newlines occur!
                                        â£¿â£¿â£¿â£·â €â €â €â¡¸â €â €â €â£ â£¿â£¿â£¶â£¤â£¤â£€â¡€â €â €â €â €â¢€â£´â£¿â£¿â£¿â£¿â¡‡
                                        â£¿â£¿â£¿â â €â €â¢€â¡‡â¢€â£ â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¶â¡žâ ‹â¢¸â£¿â£¿â£¿â£¿â¡‡ did somebody say whitespaces?. . . sounds empty to me!
                                        â£¿â¡¿â ƒâ €â â ¶â£¿â¡¿â¢»â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£žâ¢»â£¿â£¿â£¿â£¿â¡‡      [         ] < - a bunch of whitespace!
    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    The otaku_ahrii_whitespace_tokenizer() is a function built to tokenize whitespaces into tokens found within the input given to the method
    whitespace_tokenize from the nltk module, this can be used to find whitespace and the final result is to create a list as the final output with
    all the tokens inside of that array. the difference between just tokenizing it normally is that with the 'nltk.whitespace_tokenize().tokenize()'
    the method will toekenize words on spaces, tabs, newlines!

    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """
    # import module
    import nltk
    
    # tokenizing stage
    tokens = nltk.WhitespaceTokenizer().tokenize(sentencedata)
    
    # display tokens
    # display sentence tokens stage (optional)
    print('The data you have provided has been tokenized via using the nltk module:')
    print('%s whitespaces found within the text file!' %(len(tokens)))
    return print('tokens found: %s' %(tokens)) # final return value of method/func()

# tokenizes tweets
def otaku_ahrii_tweet_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
            â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–€â–ˆâ–€â€ƒâ–ˆâ–‘â–ˆâ–‘â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€â€ƒâ–€â–ˆâ–€   â–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ â–„â–€â€ƒâ–€â–„
            â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–‘â–ˆâ–‘â€ƒâ–€â–„â–€â–„â–€â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–‘â–ˆâ–‘    â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„ â–€â–„â€ƒâ–„â–€


                                                        â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                            â¡†â£â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•â …â¢—â¢•â¢•â¢•â¢•â¢•â¢•â¢•â •â •â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•
                                            â¢â¢•â¢•â¢•â¢•â¢•â£•â¢•â¢•â •â â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•â …â¡„â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•â¢•
                                            â¢•â¢•â¢•â¢•â¢•â …â¢—â¢•â •â£ â „â£—â¢•â¢•â •â¢•â¢•â¢•â •â¢ â£¿â â¢•â¢•â¢•â ‘â¢•â¢•â µâ¢•
                                            â¢•â¢•â¢•â¢•â â¢œâ •â¢â£´â£¿â¡‡â¢“â¢•â¢µâ¢â¢•â¢•â •â¢â£¾â¢¿â£§â ‘â¢•â¢•â „â¢‘â¢•â …â¢•
                                            â¢•â¢•â µâ¢â ”â¢â£¤â£¤â£¶â£¶â£¶â¡â£•â¢½â â¢•â •â£¡â£¾â£¶â£¶â£¶â£¤â¡â¢“â¢•â „â¢‘â¢…â¢‘
                                            â â£§â „â£¶â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£·â£”â¢•â¢„â¢¡â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¦â¡‘â¢•â¢¤â ±â¢
                                            â¢ â¢•â …â£¾â£¿â ‹â¢¿â£¿â£¿â£¿â ‰â£¿â£¿â£·â£¦â£¶â£½â£¿â£¿â ˆâ£¿â£¿â£¿â£¿â â¢¹â£·â£·â¡…â¢
                                            â£”â¢•â¢¥â¢»â£¿â¡€â ˆâ ›â ›â â¢ â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¡€â ˆâ ›â ›â â „â£¼â£¿â£¿â¡‡â¢”
                                            â¢•â¢•â¢½â¢¸â¢Ÿâ¢Ÿâ¢–â¢–â¢¤â£¶â¡Ÿâ¢»â£¿â¡¿â »â£¿â£¿â¡Ÿâ¢€â£¿â£¦â¢¤â¢¤â¢”â¢žâ¢¿â¢¿â£¿â â¢•
                                            â¢•â¢•â …â£â¢•â¢•â¢•â¢•â¢•â£¿â£¿â¡„â ›â¢€â£¦â ˆâ ›â¢â£¼â£¿â¢—â¢•â¢•â¢•â¢•â¢•â¢•â¡â£˜â¢•  Tokenize tweets!
                                            â¢•â¢•â …â¢“â£•â£•â£•â£•â£µâ£¿â£¿â£¿â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£•â¢•â¢•â¢•â¢•â¡µâ¢€â¢•â¢•
                                            â¢‘â¢•â ƒâ¡ˆâ¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¢ƒâ¢•â¢•â¢•      tweet tweet!
                                            â£†â¢•â „â¢±â£„â ›â¢¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ¿â¢â¢•â¢•â •â¢
                                            â£¿â£¦â¡€â£¿â£¿â£·â£¶â£¬â£â£›â£›â£›â¡›â ¿â ¿â ¿â ›â ›â¢›â£›â£‰â£­â£¤â£‚â¢œâ •â¢‘â£¡â£´â£¿          elon musk would be proud!

    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    the nltk.tweetTokenizer will search for tweets from the given input and return them as token values within a list. you may also set arguements to the 
    tweetTokenizer for matching an array of conditions such as phone numbers, remove the twitter handles from the given input and the preserve case
    postional argument that indicates wheter to preserve the capitalization of a given input from tweets.
    
    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """
    # import module
    import nltk
    from nltk.tokenize import TweetTokenizer
    
    # create tokenizer stage
    twitter_tokenizer = TweetTokenizer()
    
    # create tokens
    tokens = nltk.TweetTokenizer(preserve_case=True, reduce_len=False, strip_handles=False, match_phone_numbers=True).tokenize(sentencedata)
    
    # display tokens
    # display sentence tokens stage (optional)
    print('The data on tweets you have provided have been tokenized via using the nltk module (nltk.TweetTokenizer().tokenize(sentencedata)):')
    print('%s tweets found within the text file!' %(len(tokens)))
    return print('token tweets found: %s' %(tokens)) # final return value of method/func()

def otaku_ahrii_lemmatizer() -> Type[str]:
        
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
            â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–ˆâ–‘â–ˆâ–‘â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒ â€ƒâ–ˆâ–‘â–‘â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–„â–€â–ˆâ€ƒâ–ˆâ–€â–„â–€â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ
            â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–€â–„â–€â–„â–€â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–„â–€â€ƒ â€ƒâ–ˆâ–„â–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–‘â–ˆâ€ƒâ–ˆâ–‘â–€â–‘â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„

                                                        â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                            â£¿â£¿â£¿â£¿â£¿â£¿â Ÿâ ‹â â£€â£¤â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ¢¿â£¿â£¿
                                            â£¿â£¿â£¿â£¿â ‹â â €â €â ºâ ¿â¢¿â£¿â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˜â »â£¿
                                            â£¿â£¿â¡Ÿâ â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£¤â£¤â£¤â£¤â €â €â €â €â €â£¤â£¦â£„â €â €
                                            â£¿â¡Ÿâ €â €â €â €â €â €â €â €â €â €â¢€â£¤â£¶â£¿â â£¿â£¿â£¿â£¿â£¿â£â €â €â €â ›â ™â ›â ‹â €â €
                                            â¡¿â €â €â €â €â €â €â €â €â¡€â €â£°â£¿â£¿â£¿â£¿â¡„â ˜â£¿â£¿â£¿â£¿â£·â „â €â €â €â €â €â €â €â €
                                            â¡‡â €â €â €â €â €â €â €â ¸â ‡â£¼â£¿â£¿â£¿â£¿â£¿â£·â£„â ˜â¢¿â£¿â£¿â£¿â£…â €â €â €â €â €â €â €â €    
                                            â â €â €â €â£´â£¿â €â£â££â£¸â£¿â£¿â£¿â£¿â£¿â Ÿâ ›â ›â €â Œâ »â£¿â£¿â£¿â¡„â €â €â €â €â €â €â €
                                            â €â €â €â£¶â£®â£½â£°â£¿â¡¿â¢¿â£¿â£¿â£¿â£¿â£¿â¡€â¢¿â£¤â „â¢ â£„â¢¹â£¿â£¿â£¿â¡†â €â €â €â €â €â €            
                                            â €â €â €â£¿â£¿â£¿â£¿â£¿â¡˜â£¿â£¿â£¿â£¿â£¿â£¿â ¿â£¶â£¶â£¾â£¿â£¿â¡†â¢»â£¿â£¿â ƒâ¢ â –â ›â£›â£·â €
                                            â €â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¿â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£®â£â¡»â ¿â ¿â¢ƒâ£„â£­â¡Ÿâ¢€â¡Žâ£°â¡¶â£ªâ£¿â €        ÊŸá´‡á´á´á´€á´›Éªá´¢á´€á´›Éªá´É´ Éªs á´œsá´‡á´… ÉªÉ´ sá´‡á´€Ê€á´„Êœ á´‡É´É¢ÉªÉ´á´‡s
                                            â €â €â ˜â£¿â£¿â£¿â Ÿâ£›â »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¿â£¿â£¿â¡¿â¢â£¾â£¿â¢¿â£¿â£¿â â €                ðš›ðšŽðšðšžðšŒðšŽ ðšŠðš—ðš ðšðš’ðš—ðš ðš›ðš˜ðš˜ðš ðš•ðšŽðš–ðš–ðšŠ   
                                            â €â €â €â£»â£¿â¡Ÿâ ˜â ¿â ¿â Žâ »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£µâ£¿â£¿â §â£·â Ÿâ â €â €            
                                            â¡‡â €â €â¢¹â£¿â¡§â €â¡€â €â£€â €â ¹â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‹â¢°â£¿â €â €â €â €                 Dogs is an inflection word of the word dogs!
                                            â¡‡â €â €â €â¢»â¢°â£¿â£¶â£¿â¡¿â ¿â¢‚â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â£»â£¿â£¿â£¿â¡â €â €â â €â €â €â €            
                                            â£·â €â €â €â €â ˆâ ¿â Ÿâ£â£´â£¾â£¿â£¿â ¿â ¿â£›â£‹â£¥â£¶â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â € â£¿â¡€              reduce words down to it's lemma purest form!
                                                â‚áµ£â‚œáµ¢ð†‘áµ¢ð„´áµ¢â‚â‚— áµ¢â‚™â‚œâ‚‘â‚—â‚—áµ¢gâ‚‘â‚™ð„´â‚‘ áµ¤â‚›â‚‘â‚› áµ¢â‚œ â‚â‚› wâ‚‘â‚—â‚—
    

    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    lemmatization allows you to reduce the inflection of words in an input , sentence, article or whatever contextual data you pass, it 
    is very widely used in CHAT AI and serach engines to find a better search result for the root word.
    
    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """ 
    
    # import module
    import nltk, sys
    from nltk.stem.wordnet import WordNetLemmatizer
    global response

    # create lemmatizer
    lemmatzier = WordNetLemmatizer()
    
    # user input conditions
    while True:
        def lemmatize():
            
            # lemmatize users word
            response =  input('Select a word you would like to return the lemma for of >')
            print()
            lemma =  lemmatzier.lemmatize(word=response)

            # flow
            if response.isalpha(): # return lemma if input is str
            
                # display lemmatized input
                print('The lemma of the word %s is %s! \n' %(response, lemma))

                # re-prompt
                while True:
                    r = input('would you like to lemmatize another word? [Y/n]')
                    print()
                    
                    # re-prompt logic
                    if r == 'Y':
                        lemmatize() # recycle the function
                    elif r == 'n':
                        break # escape the loop    
                    else:
                        print('please enter a valid selection [Y/n] \n')
                        continue
    
            else:
                print('enter a word and try again! \n')
        lemmatize()
        break # escape final iterator

    # return value of method
    return None

# TODO experiement with regex tokenizers more
def otaku_ahrii_regex_tokenizer() -> Type[str]:
    
    """ < - otaku_ahrii_whitespace_tokenizer() info click to open! 
:ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    
            â–ˆâ–€â–ˆâ€ƒâ–€â–ˆâ–€â€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–‘â–ˆâ€ƒâ–„â–€â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–€â€ƒâ–€â–„â–€â€ƒ â€ƒâ–€â–ˆâ–€â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–„â–€â€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–„â–‘â–ˆâ€ƒâ–ˆâ€ƒâ–€â–ˆâ€ƒâ–ˆâ–€â–€â€ƒâ–ˆâ–€â–ˆ
            â–ˆâ–„â–ˆâ€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ–€â–„â€ƒâ–ˆâ–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆ   â–ˆâ–€â–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–ˆâ€ƒ â€ƒâ–‘â–ˆâ–‘â€ƒâ–ˆâ–„â–ˆâ€ƒâ–ˆâ–‘â–ˆâ€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–‘â–€â–ˆâ€ƒâ–ˆâ€ƒâ–ˆâ–„â€ƒâ–ˆâ–ˆâ–„â€ƒâ–ˆâ–€â–„

                                                        â¼•ã„–á—ªðŸ—â€ƒâ»ä¸«â€ƒáŽ¶ðŸ—ð“ðŸ—ä¸‚è® ä¸‚áŽ¶è® å°º
                                            â£¿â£¿â£¿â£¿â£¿â£¿â Ÿâ ‹â â£€â£¤â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ¢¿â£¿â£¿
                                            â£¿â£¿â£¿â£¿â ‹â â €â €â ºâ ¿â¢¿â£¿â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˜â »â£¿
                                            â£¿â£¿â¡Ÿâ â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£¤â£¤â£¤â£¤â €â €â €â €â €â£¤â£¦â£„â €â €
                                            â£¿â¡Ÿâ €â €â €â €â €â €â €â €â €â €â¢€â£¤â£¶â£¿â â£¿â£¿â£¿â£¿â£¿â£â €â €â €â ›â ™â ›â ‹â €â €
                                            â¡¿â €â €â €â €â €â €â €â €â¡€â €â£°â£¿â£¿â£¿â£¿â¡„â ˜â£¿â£¿â£¿â£¿â£·â „â €â €â €â €â €â €â €â €
                                            â¡‡â €â €â €â €â €â €â €â ¸â ‡â£¼â£¿â£¿â£¿â£¿â£¿â£·â£„â ˜â¢¿â£¿â£¿â£¿â£…â €â €â €â €â €â €â €â €    
                                            â â €â €â €â£´â£¿â €â£â££â£¸â£¿â£¿â£¿â£¿â£¿â Ÿâ ›â ›â €â Œâ »â£¿â£¿â£¿â¡„â €â €â €â €â €â €â €
                                            â €â €â €â£¶â£®â£½â£°â£¿â¡¿â¢¿â£¿â£¿â£¿â£¿â£¿â¡€â¢¿â£¤â „â¢ â£„â¢¹â£¿â£¿â£¿â¡†â €â €â €â €â €â €            
                                            â €â €â €â£¿â£¿â£¿â£¿â£¿â¡˜â£¿â£¿â£¿â£¿â£¿â£¿â ¿â£¶â£¶â£¾â£¿â£¿â¡†â¢»â£¿â£¿â ƒâ¢ â –â ›â£›â£·â €
                                            â €â €â¢¸â£¿â£¿â£¿â£¿â£¿â£¿â£¾â£¿â£¿â£¿â£¿â£¿â£¿â£®â£â¡»â ¿â ¿â¢ƒâ£„â£­â¡Ÿâ¢€â¡Žâ£°â¡¶â£ªâ£¿â €   
                                            â €â €â ˜â£¿â£¿â£¿â Ÿâ£›â »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£·â£¿â£¿â£¿â¡¿â¢â£¾â£¿â¢¿â£¿â£¿â â €                  
                                            â €â €â €â£»â£¿â¡Ÿâ ˜â ¿â ¿â Žâ »â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£µâ£¿â£¿â §â£·â Ÿâ â €â €            
                                            â¡‡â €â €â¢¹â£¿â¡§â €â¡€â €â£€â €â ¹â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â£¿â ‹â¢°â£¿â €â €â €â €             
                                            â¡‡â €â €â €â¢»â¢°â£¿â£¶â£¿â¡¿â ¿â¢‚â£¿â£¿â£¿â£¿â£¿â£¿â£¿â¢¿â£»â£¿â£¿â£¿â¡â €â €â â €â €â €â €         
                                            â£·â €â €â €â €â ˆâ ¿â Ÿâ£â£´â£¾â£¿â£¿â ¿â ¿â£›â£‹â£¥â£¶â£¿â£¿â£¿â£¿â£¿â €â €â €â €â €â €â €â € â£¿â¡€           
                                                â‚áµ£â‚œáµ¢ð†‘áµ¢ð„´áµ¢â‚â‚— áµ¢â‚™â‚œâ‚‘â‚—â‚—áµ¢gâ‚‘â‚™ð„´â‚‘ áµ¤â‚›â‚‘â‚› áµ¢â‚œ â‚â‚› wâ‚‘â‚—â‚—

    ðŸ‡¬â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡ªâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡¾â€‹â€‹â€‹â€‹â€‹ðŸ‡³â€‹â€‹â€‹â€‹â€‹ðŸ‡´â€‹â€‹â€‹â€‹â€‹ðŸ‡µâ€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹ðŸ‡®â€‹â€‹â€‹â€‹â€‹ðŸ‡¸â€‹â€‹â€‹â€‹â€‹
    find regex patterns and return them as tokens within a array.
    
    :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.: :ï½¥ï¾Ÿâœ§:ï½¥.â˜½Ëšï½¡ ï½¥ï¾Ÿâœ§:ï½¥.:
    """ 
    
    # import nltk
    import nltk
    from nltk.tokenize import RegexpTokenizer
    
    # pattern
    gg_pattern = r'genesisgir'
    
    # display tokens
    tokens = nltk.regexp_tokenize(text= sentencedata, pattern=gg_pattern)
    return print(tokens)

def otaku_ahrii_line_tokenizer() -> Type[str]:
    
    import nltk
    tokens = nltk.line_tokenize(text=sentencedata)
    return print(tokens)

# function call list
openfile()

# TODO create more tokenizers
# tokenize words, sentences etc.
#otaku_ahrii_word_tokenizer() 
#otaku_ahrii_sentence_tokenizer()
#otaku_ahrii_whitepace_tokenizer()
#otaku_ahrii_tweet_tokenizer()
#otaku_ahrii_lemmatizer()
#otaku_ahrii_regex_tokenizer()
#otaku_ahrii_line_tokenizer()